# -*- coding: utf-8 -*-
"""healthy-diet-recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14oryhf2SBfLdIa5KzDhwSGUYJgvRjDdw

# Food Recommendation System
"""

import sklearn
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from tqdm import tqdm_notebook as tqdm
from gensim.models import KeyedVectors
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

plt.style.use('fivethirtyeight')
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)

temp = pd.read_csv('../input/foodcom-recipes-and-reviews/recipes.csv')
temp = temp[['Name', 'Description']]
temp['Name'] = temp['Name'].str.lower()
temp.head()

df = pd.read_csv('../input/ir-project/food_sample.csv')
df = df.merge(temp, how='left', on='Name')
df.head()

df['text'] = df['Name'].astype(str)+' '+df['Category'].astype(str)+' '+df['Veg/Non-Veg'].astype(str)+' '+df['Nutrient'].astype(str)+' '+df['Disease'].astype(str)+' '+df['Diet'].astype(str)+' '+df['Ingredients'].astype(str)+' '+df['Description'].astype(str)
df.head()

"""## Preprocessing:"""

def convert_lower_case(data):
    return np.char.lower(data)

def remove_punctuation(data):
    symbols = """!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n\r"""
    for i in range(len(symbols)):
        data = np.char.replace(data, symbols[i], ' ')
        data = np.char.replace(data, "  ", " ")
    data = np.char.replace(data, ',', '')
    return data

def remove_stop_words(data):
    words = word_tokenize(str(data))
    res = ' '.join([word for word in words if word not in cachedStopWords])
    return np.char.strip(res)

def tf(word, counter):
    return counter[word] / len(counter)

def tf_binary(word, counter):
    if counter[word]>0:
        return 1
    return 0

def tf_rawcount(word, counter):
    return counter[word]

def tf_lognorm(word, counter):
    return math.log(1+counter[word])

def idf(word, postings):
    return math.log(len(postings) / (1 + postings[word]))

def lemmatization(data):
    lemmatizer = WordNetLemmatizer()
    
    tokens = word_tokenize(str(data))
    new_text = ""
    for w in tokens:
        new_text = new_text + " " + lemmatizer.lemmatize(w)
    return np.char.strip(new_text)

def preprocess(data):
    data = convert_lower_case(data)
    data = remove_punctuation(data)
    data = remove_stop_words(data)
    data = lemmatization(data)
    return data

import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

import re
import numpy as np
from tqdm import tqdm
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

cachedStopWords = stopwords.words("english")

def clean_text(data_df):
    for index, row in tqdm(data_df.iterrows()):
        sample = row['text']
        data_df.loc[index, 'text'] = str(preprocess(sample))
    return data_df

df = clean_text(df.copy())
df.head()

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(" ".join(df.text))
plt.imshow(wc , interpolation = 'bilinear')
plt.savefig('wordcloud.png')

"""## PipeLine Stage 1:

### Actual Values
"""

df.loc[df['text'].str.contains('chinese')]

"""### BERT Embeddings:"""

!pip -q install sentence_transformers

from sentence_transformers import SentenceTransformer

bert = SentenceTransformer('bert-base-nli-mean-tokens')
bert_embeddings = bert.encode(df['text']).tolist()

def filter_bert(data, embeddings, input_, n=100):
    df = data.copy()
    
    input_ = str(preprocess(input_))
    processed_input = bert.encode([input_]).tolist()

    df['Scores'] = cosine_similarity(processed_input, embeddings).reshape(-1)

    filtered_df = df.sort_values('Scores', ascending=False).head(n).reset_index(drop=True)
    return filtered_df

yo = filter_bert(df, bert_embeddings, 'chinese')
yo.head()

"""### Tf-Idf Vectorizer"""

tfidf_vectorizer = TfidfVectorizer()
tfidf_embeddings = tfidf_vectorizer.fit_transform(df['text'])
tfidf_embeddings = tfidf_embeddings.toarray()

def filter_tfidf(data, embeddings, input_, n=100):
    df = data.copy()
    
    input_ = str(preprocess(input_))
    processed_input = tfidf_vectorizer.transform([input_])
    
    df['Scores'] = cosine_similarity(processed_input, embeddings).reshape(-1)

    filtered_df = df.sort_values('Scores', ascending=False).head(n).reset_index(drop=True)
    return filtered_df

yo = filter_tfidf(df, tfidf_embeddings, 'chinese')
yo.head()

"""### Count Vectorizer"""

count_vectorizer = CountVectorizer()
count_embeddings = count_vectorizer.fit_transform(df['text'])
count_embeddings = count_embeddings.toarray()

def filter_count(data, embeddings, input_, n=100):
    df = data.copy()
    
    input_ = str(preprocess(input_))
    processed_input = count_vectorizer.transform([input_])
    
    df['Scores'] = cosine_similarity(processed_input, embeddings).reshape(-1)

    filtered_df = df.sort_values('Scores', ascending=False).head(n).reset_index(drop=True)
    return filtered_df

yo = filter_count(df, count_embeddings, 'chinese')
yo.head()

"""### FastText Embeddings"""

FILE_PATH = '../input/fasttext-wikinews/wiki-news-300d-1M.vec'
keyed_vec = KeyedVectors.load_word2vec_format(FILE_PATH)

def mean_fasttext(arr, embedding_dim=300):
    mean_vectors = []
    for document in tqdm(arr):
        tokens = nltk.tokenize.word_tokenize(document)
        vectors = [keyed_vec.get_vector(token) for token in tokens if token in keyed_vec.key_to_index.keys()]
        if vectors:
            mean_vec = np.vstack(vectors).mean(axis=0)
            mean_vectors.append(mean_vec)
        else:
            mean_vectors.append(np.zeros(embedding_dim))
    embedding = np.vstack(mean_vectors)
    return embedding

fasttext_embeddings = mean_fasttext(df['text'])

def filter_fasttext(data, embeddings, input_, n=100):
    df = data.copy()
    
    input_ = str(preprocess(input_))
    processed_input = mean_fasttext([input_])
    
    df['Scores'] = cosine_similarity(processed_input, embeddings).reshape(-1)

    filtered_df = df.sort_values('Scores', ascending=False).head(n).reset_index(drop=True)
    return filtered_df

yo = filter_fasttext(df, fasttext_embeddings, 'chinese')
yo.head()

"""# SOTA Pipeline (Stage 1):"""

def filter_SOTA(data, embeddings1, embeddings2, embeddings3, embeddings4, input_, n=100):
    df = data.copy()
    
    input_ = str(preprocess(input_))
    
    processed_input = bert.encode([input_]).tolist()
    df['bert_score'] = cosine_similarity(processed_input, embeddings1).reshape(-1)
    
    processed_input = tfidf_vectorizer.transform([input_])
    df['tfidf_score'] = cosine_similarity(processed_input, embeddings2).reshape(-1)
    
    processed_input = count_vectorizer.transform([input_])
    df['count_score'] = cosine_similarity(processed_input, embeddings3).reshape(-1)
    
    processed_input = mean_fasttext([input_])
    df['fasttext_score'] = cosine_similarity(processed_input, embeddings4).reshape(-1)
    
    df['ensemble_score'] = 0.8*df['count_score'] + 0.2*df['tfidf_score']
    
    filtered_df = df.sort_values('ensemble_score', ascending=False).head(n).reset_index(drop=True)
    return filtered_df

yo = filter_SOTA(df, bert_embeddings, tfidf_embeddings, count_embeddings, fasttext_embeddings, 'chinese')
yo.head()

"""## Comparison of Baseline Models:"""

test_inputs = ['chinese', 'spicy', 'chicken', 'indian', 'sweet', 'light']

"""Encoding Names of Dishes"""

encoder = LabelEncoder()
encoder.fit(df['Name'])

actual = []
bert_preds = []
tfidf_preds = []
count_preds = []
fasttext_preds = []
SOTA_preds = []
random_preds = []

n = 10

for i in tqdm(test_inputs):
    a = list(df.loc[df['text'].str.contains(i)]['Name'][:n])
    a = list(encoder.transform(a))
    actual.append(a)
    
    a = list(filter_bert(df, bert_embeddings, i)['Name'][:n])
    a = list(encoder.transform(a))
    bert_preds.append(a)
    
    a = list(filter_tfidf(df, tfidf_embeddings, i)['Name'][:n])
    a = list(encoder.transform(a))
    tfidf_preds.append(a)
    
    a = list(filter_count(df, count_embeddings, i)['Name'][:n])
    a = list(encoder.transform(a))
    count_preds.append(a)
    
    a = list(filter_fasttext(df, fasttext_embeddings, i)['Name'][:n])
    a = list(encoder.transform(a))
    fasttext_preds.append(a)
    
    a = list(filter_SOTA(df, bert_embeddings, tfidf_embeddings, count_embeddings, fasttext_embeddings, i)['Name'][:n])
    a = list(encoder.transform(a))
    SOTA_preds.append(a)
    
    a = df.Name.sample(n).values.tolist()
    a = list(encoder.transform(a))
    random_preds.append(a)

result = pd.DataFrame()

result['actual'] = actual
result['bert_preds'] = bert_preds
result['tfidf_preds'] = tfidf_preds
result['count_preds'] = count_preds
result['fasttext_preds'] = fasttext_preds
result['SOTA_preds'] = SOTA_preds
result['random_preds'] = random_preds

result.head()

!pip -q install recmetrics

import recmetrics

bert_mark = []
for K in np.arange(1, 11):
    bert_mark.extend([recmetrics.mark(result.actual.values, result.bert_preds.values, k=K)])
bert_mark

tfidf_mark = []
for K in np.arange(1, 11):
    tfidf_mark.extend([recmetrics.mark(result.actual.values, result.tfidf_preds.values, k=K)])
tfidf_mark

count_mark = []
for K in np.arange(1, 11):
    count_mark.extend([recmetrics.mark(result.actual.values, result.count_preds.values, k=K)])
count_mark

fasttext_mark = []
for K in np.arange(1, 11):
    fasttext_mark.extend([recmetrics.mark(result.actual.values, result.fasttext_preds.values, k=K)])
fasttext_mark

SOTA_mark = []
for K in np.arange(1, 11):
    SOTA_mark.extend([recmetrics.mark(result.actual.values, result.SOTA_preds.values, k=K)])
SOTA_mark

random_mark = []
for K in np.arange(1, 11):
    random_mark.extend([recmetrics.mark(result.actual.values, result.random_preds.values, k=K)])
random_mark

"""### Comparing MAR@K Score for all Embeddings:"""

def mark_plot(mark_scores, model_names, k_range):
    #create palette
    recommender_palette = ["#ED2BFF", "#14E2C0", "#FF9F1C", "#5E2BFF","#FC5FA3"]
    sns.set_palette(recommender_palette)

    #lineplot
    mark_df = pd.DataFrame(np.column_stack(mark_scores), k_range, columns=model_names)
    ax = sns.lineplot(data=mark_df)
    plt.xticks(k_range)
    plt.setp(ax.lines,linewidth=5)

    #set labels
    ax.set_title('Mean Average Recall at K (MAR@K) Comparison')
    ax.set_ylabel('MAR@K')
    ax.set_xlabel('K')
    
    plt.savefig('mark_score.png', bbox_inches = 'tight')
    plt.show()

mark_scores = [bert_mark, tfidf_mark, count_mark, fasttext_mark, SOTA_mark, random_mark]
index = range(1,10+1)
names = ['BERT Embeddings', 'Tf-Idf Embeddings', 'Count Embeddings', 'FastText Embeddings', 'SOTA Embeddings', 'Random Embeddings']

fig = plt.figure(figsize=(15, 7))
mark_plot(mark_scores, model_names=names, k_range=index)

"""### Prediction Coverage:"""

catalog = encoder.transform(df['Name'])
bert_coverage = recmetrics.prediction_coverage(result.bert_preds.values, catalog)
tfidf_coverage = recmetrics.prediction_coverage(result.tfidf_preds.values, catalog)
count_coverage = recmetrics.prediction_coverage(result.count_preds.values, catalog)
fasttext_coverage = recmetrics.prediction_coverage(result.fasttext_preds.values, catalog)
SOTA_coverage = recmetrics.prediction_coverage(result.SOTA_preds.values, catalog)
random_coverage = recmetrics.prediction_coverage(result.random_preds.values, catalog)

coverage_scores = [bert_coverage, tfidf_coverage, count_coverage, fasttext_coverage, SOTA_coverage, random_coverage]
names = ['BERT Embeddings', 'Tf-Idf Embeddings', 'Count Embeddings', 'FastText Embeddings', 'SOTA Embeddings', 'Random Embeddings']

fig = plt.figure(figsize=(10, 7))
plt.bar(names, coverage_scores)
plt.title('Comparison of Prediction Coverage')
plt.xticks(rotation=40)
plt.savefig('preds_cov.png', bbox_inches = 'tight')
plt.show()

"""### Catalog Coverage:"""

catalog = encoder.transform(df['Name'])
bert_coverage = recmetrics.catalog_coverage(result.bert_preds.values, catalog, 10)
tfidf_coverage = recmetrics.catalog_coverage(result.tfidf_preds.values, catalog, 10)
count_coverage = recmetrics.catalog_coverage(result.count_preds.values, catalog, 10)
fasttext_coverage = recmetrics.catalog_coverage(result.fasttext_preds.values, catalog, 10)
SOTA_coverage = recmetrics.catalog_coverage(result.SOTA_preds.values, catalog, 10)
random_coverage = recmetrics.catalog_coverage(result.random_preds.values, catalog, 10)

coverage_scores = [bert_coverage, tfidf_coverage, count_coverage, fasttext_coverage, SOTA_coverage, random_coverage]
names = ['BERT Embeddings', 'Tf-Idf Embeddings', 'Count Embeddings', 'FastText Embeddings', 'SOTA Embeddings', 'Random Embeddings']

fig = plt.figure(figsize=(10, 7))
plt.bar(names, coverage_scores)
plt.title('Comparison of Catalog Coverage')
plt.xticks(rotation=40)
plt.savefig('cat_cov.png', bbox_inches = 'tight')
plt.show()

"""### Novelty Of Recommendations:"""

nov = pd.Series(encoder.transform(df['Name'])).value_counts()
pop = dict(nov)

bert_novelty = recmetrics.novelty(result.bert_preds.values, pop, len(pop), 10)
tfidf_novelty = recmetrics.novelty(result.tfidf_preds.values, pop, len(pop), 10)
count_novelty = recmetrics.novelty(result.count_preds.values, pop, len(pop), 10)
fasttext_novelty = recmetrics.novelty(result.fasttext_preds.values, pop, len(pop), 10)
SOTA_novelty = recmetrics.novelty(result.SOTA_preds.values, pop, len(pop), 10)
random_novelty = recmetrics.novelty(result.random_preds.values, pop, len(pop), 10)

coverage_scores = [bert_novelty[0], tfidf_novelty[0], count_novelty[0], fasttext_novelty[0], SOTA_novelty[0], random_novelty[0]]
names = ['BERT Embeddings', 'Tf-Idf Embeddings', 'Count Embeddings', 'FastText Embeddings', 'SOTA Embeddings', 'Random Embeddings']

fig = plt.figure(figsize=(10, 7))
plt.bar(names, coverage_scores)
plt.title('Comparison of Novelty Score')
plt.xticks(rotation=40)
plt.savefig('novelty_score.png', bbox_inches = 'tight')
plt.show()

"""# Pipeline Stage 2:"""

class Recommender:
    def __init__(self, data):
        self.df = data
        
        cat_dummies = self.df['Veg/Non-Veg'].str.get_dummies()
        nutrient_dummies = self.df['Nutrient'].str.get_dummies()
        disease_dummies = self.df['Disease'].str.get_dummies(sep=' ')
        diet_dummies = self.df['Diet'].str.get_dummies(sep=' ')
        feature_df = pd.concat([cat_dummies, nutrient_dummies, disease_dummies, diet_dummies], axis=1)
        
        self.data = feature_df
        
    def get_recommendations(self, inputs, k):
        total_features = self.data.columns
        temp = dict()
        for i in total_features:
            if i in inputs:
                temp[i] = 1
            else:
                temp[i] = 0
        
        temp = pd.DataFrame([temp])
        
        model = NearestNeighbors(n_neighbors=k, algorithm='ball_tree')
        model.fit(self.data)
        
        distances, indices = model.kneighbors(temp)
        
        results = self.df.iloc[indices[0]].reset_index(drop=True)
        results['Disance'] = distances[0]
        return results

"""# Inference:"""

filtered_df = filter_SOTA(df, bert_embeddings, tfidf_embeddings, count_embeddings, fasttext_embeddings, 'sweet')

ob = Recommender(filtered_df)
sample_input = ['veg', 'high_protien_diet', 'gluten_free_diet', 'diabeties', 'anemia', 'calcium', 'protien']

results = ob.get_recommendations(sample_input, 5)
results



